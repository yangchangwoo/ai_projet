{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a0797",
   "metadata": {},
   "source": [
    "# 다중 슬롯머신 문제(multi aramed bandit)\n",
    "\n",
    "- 환경\n",
    "    - 여러 손잡이 중 하나를 선택해 1달러를 넣고 당김\n",
    "        - 그 결과 플레이어는 1달러를 잃거나 1달러를 따게 됨\n",
    "    - 손잡이마다 승리할 확률이 정해져 있음\n",
    "        - 돈을 따려면 확률이 높은 손잡이를 당겨야 함\n",
    "    - 확률은 감춰져 있음\n",
    "    \n",
    "- 플레이어가 취할 수 있는 행동\n",
    "    - 행동에 따라 보상이 주어짐\n",
    "    - 행동 : 손잡이를 고르는 일\n",
    "    - 손잡이가 5개라면 행동의 집합은 0, 1, 2, 3, 4\n",
    "    \n",
    "- 보상\n",
    "    - 돈을 잃거나 따는 것\n",
    "        - 1달러를 따거나 1달러를 잃거나\n",
    "    - 보상의 집합은 1, -1\n",
    "    \n",
    "- 슬롯머신 문제의 특성\n",
    "    - 다중 슬롯머신 문제에는 상태가 없음\n",
    "    - 손잡이를 당긴 후에도 환경은 그대로임\n",
    "        - 손잡이도 그대로, 손잡이마다 설정된 확률도 그대로\n",
    "    - 다중 슬롯머신은 행동과 보상만 있는 단순한 문제임\n",
    "        - 강화학습의 기초개념을 익히기에 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34d2e1",
   "metadata": {},
   "source": [
    "## 탐험형 정책과 탐사형 정책\n",
    "\n",
    "- 탐험형(exploration) 정책\n",
    "    - 처음부터 끝까지 손잡이를 무작위로 선택하는 랜덤 정책(random policy)\n",
    "    - 이전 경험을 전혀 사용하지 않는 매우 비효율적인 방식\n",
    "\n",
    "- 탐사형(exploitation) 정책\n",
    "    - 몇 번 시도해보고 이후에는 그때까지 가장 높은 확률을 보인 손잡이만 계속 당기는 정책\n",
    "    - 확률이 더 높은 손잡이를 놓칠 위험이 있음\n",
    "    \n",
    "- 위의 두 정책은 양 극단에 속함\n",
    "- 강화학습에서는 둘 사이에서 균형을 잡는 것이 중요\n",
    "    - 예) 현재까지 높은 확률을 보인 손잡이를 더 자주 당기지만 일정 비율로 다른 손잡이도 시도해보는 등\n",
    "    - 이 때 다른 손잡이를 시도하는 비율이 높을수록 탐험형에 가깝고 낮을수록 탐사형에 가까움\n",
    "    \n",
    "- 슬롯머신 문제는 단순하기 때문에 연속으로 행동을 취하고 마칠때까지의 기록(에피소드)을 충분히 길게 하여 최적의 정책을 찾을 수 있음\n",
    "    - 랜덤 정책을 길게 적용하여 충분히 긴 에피소드를 수집하고 손잡이마다 돈을 따게될 확률을 계산\n",
    "        - 예) 1000달러를 가지고 랜덤 정책을 이용해 길이가 1000인 에피소드를 수집\n",
    "        - 모든 손잡이에 대해 확률을 계산하여 만약 네 번째 손잡이의 확률이 가장 높다면 수익을 최대로 보장하는 최적 정책(optimal policy)은 (0, 0, 0, 1, 0)\n",
    "        - 네 번째 손잡이를 당기는 확률을 1로 설정하면 많은 돈을 벌 수 있음\n",
    "    - 하지만 실제로는 확률이 바뀔 수도 있고 신뢰할만한 확률을 알아낼 수 있을 정도로 돈과 시간이 충분하지 않을 수 있기 때문에 더 영리한 알고리즘이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05864ffa",
   "metadata": {},
   "source": [
    "### 랜덤 정책을 쓰는 알고리즘\n",
    "\n",
    "- 알고리즘의 목표는 확률을 모른 채 플레이하면서 수익을 최대화하는 정책을 찾는 것\n",
    "- 알고리즘에 주어지는 정보는 행동을 했을 때의 보상 뿐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4da7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de80b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 손잡이 밴딧 문제 설정\n",
    "arms_profit = [0.4, 0.12, 0.52, 0.6, 0.25]\n",
    "n_arms = len(arms_profit)\n",
    "\n",
    "n_trial = 10000 # 손잡이를 당기는 횟수(에피소드 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127cddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손잡이를 당기는 행위를 시뮬레이션하는 함수(handle은 손잡이 번호)\n",
    "def pull_bandit(handle):\n",
    "    q = np.random.random()\n",
    "    if q < arms_profit[handle]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c399bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 정책을 모방하는 함수\n",
    "def random_exploration():\n",
    "    episode = []\n",
    "    \n",
    "    num = np.zeros(n_arms) # 손잡이별로 당긴 횟수\n",
    "    wins = np.zeros(n_arms) # 손잡이별 승리 횟수\n",
    "    \n",
    "    for i in range(n_trial):\n",
    "        h = np.random.randint(0, n_arms)\n",
    "        reward = pull_bandit(h)\n",
    "        episode.append([h, reward])\n",
    "        num[h] += 1\n",
    "        wins[h] += 1 if reward == 1 else 0\n",
    "        \n",
    "    return episode, (num, wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450cfd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률: ['0.4036', '0.1333', '0.5167', '0.6098', '0.2424']\n",
      "손잡이별 수익($): ['-391', '-1447', '68', '440', '-1012']\n",
      "순 수익($): -2342\n"
     ]
    }
   ],
   "source": [
    "e, r = random_exploration()\n",
    "\n",
    "print(\"손잡이별 승리 확률:\",\n",
    "      [\"%6.4f\"%(r[1][i]/r[0][i]) if r[0][i] > 0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($):\",\n",
    "     [\"%d\"%(2 * r[1][i] - r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($):\", sum(np.asarray(e)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5ed86",
   "metadata": {},
   "source": [
    "### ε-탐욕 알고리즘(greedy algorithm)\n",
    "\n",
    "- 탐욕 알고리즘 : 과거와 미래를 전혀 고려하지 않고 현재 순간의 정보만으로 선택을 하는 알고리즘 방법론\n",
    "    - 탐험형과 탐사형 중 탐사형에 치우친 방법론\n",
    "    \n",
    "- ε-탐욕 알고리즘\n",
    "    - 기본적으로는 탐욕 알고리즘\n",
    "    - ε비율만큼 탐험을 적용하여 탐험과 탐사의 균형을 추구\n",
    "    - 현재까지 파악한 승리 확률에 따라 행동을 선택하는 탐사형 방식을 사용하지만, ε 비율만큼 랜덤 정책을 적용하여 탐험형을 섞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90749a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ε-탐욕을 구현하는 함수\n",
    "def epsilon_greedy(eps):\n",
    "    episode = []\n",
    "    num = np.zeros(n_arms) # 손잡이별로 당긴 횟수\n",
    "    wins = np.zeros(n_arms) # 손잡이별 승리 횟수\n",
    "    \n",
    "    for i in range(n_trial):\n",
    "        r = np.random.random()\n",
    "        if r < eps or sum(wins) == 0: # 확률 eps 로 임의 선택\n",
    "            h = np.random.randint(0, n_arms)\n",
    "            \n",
    "        else:\n",
    "            prob = np.asarray([wins[i] / num[i] if num[i] > 0 else 0.0 for i in range(n_arms)])\n",
    "            prob = prob / sum(prob)\n",
    "            h = np.random.choice(range(n_arms), p = prob)\n",
    "        \n",
    "        reward = pull_bandit(h)\n",
    "        episode.append([h, reward])\n",
    "        num[h] += 1\n",
    "        wins[h] += 1 if reward == 1 else 0\n",
    "    return episode, (num, wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6c2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률: ['0.3944', '0.1157', '0.5259', '0.5852', '0.2330']\n",
      "손잡이별 수익($): ['-434', '-618', '146', '519', '-683']\n",
      "순 수익($): -1070\n"
     ]
    }
   ],
   "source": [
    "e, r = epsilon_greedy(0.1)\n",
    "\n",
    "print(\"손잡이별 승리 확률:\",\n",
    "      [\"%6.4f\"%(r[1][i] / r[0][i]) if r[0][i] > 0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($):\",\n",
    "      [\"%d\"%(2 * r[1][i] - r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($):\", sum(np.asarray(e)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216649c",
   "metadata": {},
   "source": [
    "- 탐험과 탐사의 균형을 추구하는 엡실론 탐욕 알고리즘은 이전 정보를 완전히 무시하는 랜덤 정책에 비해 훨씬 영리함\n",
    "- 현실 세계의 현상 또는 수학적 현상을 난수를 생성하여 시뮬레이션하는 기법을 통틀어 몬테카를로 방법(Monte Carlo method)라고 함\n",
    "    - 인공지능은 강화학습 뿐만 아니라 다양한 목적으로 몬테카를로 방법을 활용\n",
    "    - 랜덤 정책 알고리즘과 엡실론-탐욕 알고리즘을 사용한 다중 슬롯머신 문제는 다중 슬롯머신의 동작을 난수로 시뮬레이션했기 때문에 몬테카를로 방법에 속함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179df54b",
   "metadata": {},
   "source": [
    "# OpenAI의 gym라이브러리\n",
    "\n",
    "- 강화학습을 구현할 때는 OpenAI 재단이 제공하는 gym 라이브러리를 주로 사용\n",
    "- OpenAI\n",
    "    - 누구나 혜택을 받는 안전한 인공지능을 구현하려는 목표로 창립한 비영리 재단\n",
    "    \n",
    "- gym 라이브러리\n",
    "    - 강화학습으로 풀어야 하는 문제를 여러 가지 제공\n",
    "    - FrozenLake\n",
    "        - 시작점에서 출발해 한 칸씩 움직여 목표 지점에 도달하면 이기는 게임\n",
    "        - 중간에 구멍에 빠지면 지고, 밟고 지나갈 수 있는 곳은 정해져 있음\n",
    "        - 에이전트는 어떤 칸이 구멍이고 어떤 칸이 지나갈 수 있는지에 대한 정보는 주어지지 않음\n",
    "    \n",
    "    - CartPole\n",
    "        - 막대를 오래 세워놓을수록 높은 점수를 받는 문제\n",
    "        - 막대가 왼쪽으로 기울어지면 수레를 왼쪽으로 움직여 평형을 이루어야함\n",
    "        - 너무 극단적으로 많이 이동하면 오히려 회복이 불가능해짐\n",
    "    \n",
    "    - MountainCar\n",
    "        - 언덕에 있는 깃발에 도달하면 이기는 게임\n",
    "        - 반대쪽 언덕으로 충분히 올라간다음 구르는 힘으로 목적지에 도달해야함\n",
    "    \n",
    "    - FetchSlide\n",
    "        - 로봇이 손으로 검은색 퍽을 쳐서 특정 지점으로 옮기는 문제\n",
    "        - 정확한 방향을 향해 적절한 힘을 가해야 성공할 수 있음\n",
    "    \n",
    "    - Atari\n",
    "        - 1980년대에 유행하던 고전 비디오게임 수십종이 제공됨\n",
    "        - 대표적인 것은 Breakout, BankHeist, FishingDerby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa51036",
   "metadata": {},
   "source": [
    "# FrozenLake 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da974ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1288f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "[[2, 0.0, 1], [1, 0.0, 5], [2, 0.0, 1], [1, 0.0, 5], [1, 0.0, 4], [0, 0.0, 4], [2, 0.0, 5], [3, 0.0, 0], [0, 0.0, 0], [1, 0.0, 4], [2, 0.0, 5], [2, 0.0, 1], [3, 0.0, 1], [3, 0.0, 1], [2, 0.0, 2], [0, 0.0, 1], [3, 0.0, 1], [2, 0.0, 2], [3, 0.0, 2], [0, 0.0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 환경 불러오기\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery = False, render_mode = \"human\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "n_trial = 20\n",
    "\n",
    "# 에피소드 수집\n",
    "env.reset() # env를 리셋하여 에피소드 새로 시작\n",
    "episode = [] # 수집한 에피소드를 저장할 리스트\n",
    "\n",
    "for i in range(n_trial): # 행동을 n_trial번 반복\n",
    "    action = env.action_space.sample() # 행동을 취함(랜덤 선택)\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # 보상을 받고 상태가 바뀜\n",
    "    episode.append([action, reward, obs]) # 행동, 보상, 다음 상태\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "print(episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5c63f",
   "metadata": {},
   "source": [
    "- Discrete(16)(observation_space)\n",
    "    - 16개의 상태가 있다는 뜻\n",
    "    - 4 x 4 의 격자 위의 어느 곳에 있는지를 표현\n",
    "    - FrozenLake 환경\n",
    "        - S F F F\n",
    "        - F H F H\n",
    "        - F F F H\n",
    "        - H F F G\n",
    "        \n",
    "- Discrete(4)(action_space)\n",
    "    - 상하좌우로 이동하는 네 가지 행동이 있다는 뜻\n",
    "    - 0 : Left\n",
    "    - 1 : Down\n",
    "    - 2 : Right\n",
    "    - 3 : Up\n",
    "    \n",
    "- episode\n",
    "    - 예) [1, 0.0, 4] : 행동 1을 하고 보상 0.0을 받고 상태 4로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfeff80",
   "metadata": {},
   "source": [
    "# 계산 모형\n",
    "\n",
    "- 강화학습이 풀어야 하는 문제는 환경(environment)로 정의됨\n",
    "- 환경\n",
    "    - 상태의 종류, 행동의 종류, 보상의 종류를 지정하며 행동을 취했을 때 발생하는 상태 변환을 지배하는 규칙을 포함하는 개념\n",
    "    - 이런 정보를 통틀어서 마르코프 결정 프로세스(MDP: Markov Decision Process)라고 함\n",
    "    \n",
    "- 상태와 행동, 보상\n",
    "    - 에이전트와 환경은 밀접하게 상호작용하며, 에이전트는 환경이 제공하는 상태와 보상에 따라 행동을 취함\n",
    "    - 슬롯머신 문제에서는 기계앞에서 도박을 하는 사람, FrozenLake에서는 호수를 건너는 사람이 에이전트가 됨\n",
    "    \n",
    "- 슬롯머신과 FrozenLake 문제를 MDP 에 대입하면\n",
    "    - 슬롯머신 문제\n",
    "        - 상태 집합 : 공집합\n",
    "        - 행동 집합 : {손잡이0, 손잡이1, 손잡이2, 손잡이3, 손잡이4}\n",
    "        - 보상 집합 : {1, -1}(즉시 보상)\n",
    "        \n",
    "    - FrozenLake 문제\n",
    "        - 상태 집합 : {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
    "        - 행동 집합 : {0(Left), 1(Down), 2(Right), 3(Up)}\n",
    "        - 보상 집합 : {0, 1}(지연된 보상)\n",
    "        \n",
    "        \n",
    "- 보상이 주어지는 시점\n",
    "    - 슬롯머신 문제에서는 손잡이를 당겨 행동을 취하면 즉시 보상이 주어짐\n",
    "    - Frozenlake에서는 목표 지점에 도달하면 1이라는 보상이 주어지는데, 그 외에는 0이 주어짐\n",
    "        - 중간에 0이라는 보상이 주어지는 것은 보상 총액에 영향을 미치지 않기 때문에 의미가 없음\n",
    "        - FrozenLake와 같은 경우를 지연된 보상(delayed reward)라고 함\n",
    "        - 바둑과 장기도 지연된 보상 체계\n",
    "        \n",
    "- 상태 전이\n",
    "    - 어떤 상태에서 행동을 취하면 새로운 상태로 전이\n",
    "        - 예) FrozenLake 문제에서 1의 칸에서 2(Right)라는 행동을 취한다면\n",
    "            - 주어지는 보상 : 0\n",
    "            - 새로운 상태 : 2라는 칸으로 이동하기 때문에 2\n",
    "                - 이 때 새로운 상태는 항상 2임\n",
    "                - 100%확률로 새로운 상태가 정해지는 환경을 결정론적 환경(deterministic environment)라고 함\n",
    "                    - 바둑, 장기, 비디오 게임 등은 모두 결정론적 환경에 해당\n",
    "                - FrozenLake 문제에서 얼음 바닥이 미끄러워 오른쪽 행동을 취했는데 다른 방향으로 이동하게 되는 상황을 허용한다면\n",
    "                    - 스토케스틱 환경(stochastic environment)이 됨\n",
    "                    - is_slippery = True 로 설정하면 스토케스틱 환경이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b354c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
