{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9791ddb",
   "metadata": {},
   "source": [
    "# 강화학습(Reinforcement Learning)\n",
    "\n",
    "- 20세기 행동심리학에서 강화(Reinforcement)라는 개념이 등장\n",
    "    - 동물이 시행착오를 통해 학습하는 방법 중 하나\n",
    "        - 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것\n",
    "        - 점차 좋은 보상을 얻게 해주는 행동을 점점 더 많이 하게됨\n",
    "            - 보상이 왜 나오는지에 대해 이해하는 것은 아님\n",
    "    - 강화는 사람에게도 익숙한 개념임\n",
    "        - 처음 걷는 것을 배울 때 걷는 법을 배우지 않아도 스스로 이것저것 시도해보면서 걷게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72538ac",
   "metadata": {},
   "source": [
    "# 머신러닝에서의 강화학습\n",
    "\n",
    "<img src = \"./image/agent_env.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13e183",
   "metadata": {},
   "source": [
    "- 지도학습, 비지도학습과 다르게 정답이 주어지지도 않고 주어진 데이터에 대해 학습하지도 않음\n",
    "\n",
    "- 강화학습은 보상(Reward)을 통해 학습\n",
    "    - 보상 : 컴퓨터가 선택한 행동에 대한 환경의 반응\n",
    "    \n",
    "- 에이전트(agent) : 강화학습을 통해 스스로 학습하는 컴퓨터\n",
    "    - 에이전트는 환경에 대해 사전지식이 없는 상태에서 학습\n",
    "    - 자신이 놓인 환경에서 자신의 상태를 인식한 후 행동\n",
    "    - 환경은 에이전트에게 행동에 대한 보상을 주고 다음 상태를 알려줌\n",
    "    - 보상을 통해 에이전트는 어떤 행동이 좋은지 간접적으로 알게됨\n",
    "        - 강화학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 \"최적의 행동양식, 또는 정책\"을 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba11c0",
   "metadata": {},
   "source": [
    "## 강화학습 문제\n",
    "\n",
    "- 강화학습은 사람처럼 환경과 상호작용하면서 스스로 학습하는 방식\n",
    "- 결정을 순차적으로 내려야하는 문제에 주로 적용됨\n",
    "    - 현재 위치에서 행동을 한 번 선택하는 것이 아니라 지속적으로 선택해야함\n",
    "- 일반적으로 순차적으로 결정을 내리는 문제는 다이내믹 프로그래밍 또는 진화 알고리즘을 적용할 수 있지만 강화학습은 다이내믹 프로그래밍과 진화 알고리즘의 한계를 극복할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef6d57",
   "metadata": {},
   "source": [
    "### 순차적 행동 결정 문제\n",
    "\n",
    "- 순차적 행동 결정 문제를 수학적으로 표현하기 위한 가장 간단한 방법은 문제를 수치화하는 것\n",
    "    - 예) 시험을 통해 수학 실력을 수치화\n",
    "        - 시험점수가 학생들의 진정한 수학 실력을 나타내주지 않을 수 있지만 대체적으로 수학 실력과 비례함\n",
    "        - 만약 수학 실력을 수치화하지 않는다면 수학 점수를 높이기 위한 전략을 세우기 어려움\n",
    "        \n",
    "- 강화학습도 마찬가지로 에이전트가 학습하고 발전하려면 문제를 수학적으로 표현할 수 있어야함\n",
    "    - 이 때 사용하는 방법이 MDP(Markov Decision Process)\n",
    "        - 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a809e1",
   "metadata": {},
   "source": [
    "#### 순차적 행동 결정 문제의 구성 요소\n",
    "\n",
    "- 수학적으로 정의된 문제는 아래의 구성 요소를 가짐\n",
    "\n",
    "1. 상태(state)\n",
    "    - 에이전트의 상태\n",
    "    - 어떠한 정적인 요소를 포함하며 에이전트가 움직이는 속도와 같은 동적인 요소 또한 포함하는 표현\n",
    "        - 에이전트가 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야함\n",
    "        - 예) 탁구를 치는 에이전트라면 탁구공의 위치 뿐만아니라 탁구공의 속도, 가속도 등의 정보가 필요함\n",
    "\n",
    "2. 행동(action)\n",
    "    - 에이전트가 어떠한 상태에서 취할 수 있는 행동\n",
    "        - 게임에서의 행동이라면 게임기를 통해 줄 수 있는 입력\n",
    "        \n",
    "    - 학습이 되지 않은 에이전트는 어떤 행동이 좋은 행동인지에 대한 정보가 없어서 무작위로 행동함\n",
    "    - 하지만 학습하면서 특정한 행동들을 취할 확률을 높임\n",
    "    - 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌\n",
    "\n",
    "3. 보상(reward)\n",
    "    - 강화학습을 다른 머신러닝 기법과 다르게 만들어주는 가장 핵심적인 요소\n",
    "    - 에이전트가 학습할 수 있는 유일한 정보\n",
    "    - 보상을 통해 에이전트는 자신의 행동을 평가\n",
    "    - 보상은 에이전트에 속하지 않는 환경의 일부\n",
    "        - 에이전트는 어떤 상황에서 얼마의 보상이 나오는지 알지 못함\n",
    "\n",
    "4. 정책(policy)\n",
    "    - 순차적 행동 결정 문제에서 구해야 할 답\n",
    "    - 특정 상태가 아닌 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것\n",
    "    - 제일 좋은 정책은 최적 정책(optimal policy)라고 하며 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff382e",
   "metadata": {},
   "source": [
    "# 강화학습의 예시(브레이크아웃)\n",
    "\n",
    "<img src = \"./image/breakout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a25596",
   "metadata": {},
   "source": [
    "- 아타리의 고전 게임\n",
    "    - 딥마인드의 Playing Atari with deep Reinforcement Learning 논문에서 아타리 게임에 강화학습을 적용\n",
    "    \n",
    "- 브레이크 아웃의 MDP와 학습 방법\n",
    "    1. MDP\n",
    "        - 브레이크아웃에서 에이전트가 환경으로부터 받아들이는 상태는 게임 화면\n",
    "        - 따라서 에이전트가 상황을 파악할 수 있도록 같은 화면을 연속으로 4개를 받아 하나의 상태로 에이전트에게 제공\n",
    "            - 게임화면은 흑백화면이기 때문에 2차원 픽셀 데이터\n",
    "            \n",
    "        - 행동 : 제자리, 왼쪽, 오른쪽, 발사가 가능하고 발사는 게임을 시작할 때 사용\n",
    "            - 사실상 게임 도중에는 제자리, 왼쪽, 오른쪽만 가능\n",
    "            \n",
    "        - 보상 : 벽돌이 하나씩 깨질 때 마다 보상을 +1씩 받고 더 위쪽을 깰수록 더 큰 보상을 받음\n",
    "            - 아무 것도 깨지 않을 때는 보상 0, 공을 놓쳐서 목숨을 잃을 경우에는 보상 -1\n",
    "            \n",
    "    2. 학습\n",
    "        - 처음에는 에이전트가 게임이나 상황을 모르기 때문에 무작위로 제자리, 왼쪽, 오른쪽으로 움직임\n",
    "        - 그러다가 우연히 공을 쳐서 벽돌을 깨면 게임(환경)으로부터 +1의 보상을 받고, 공을 놓친다면 -1의 보상을 받음\n",
    "        - 위 과정의 반복을 통해 어떻게 해야 공을 떨어뜨리지 않고 벽을 깰 수 있는지 학습\n",
    "        - 게임 화면에서 왼쪽 상단의 숫자는 누적되는 보상\n",
    "            - 에이전트는 게임으로부터 즉각적인 보상을 받지만 에이전트의 목표는 즉각적인 보상이 아닌 누적되는 보상의 합을 최대화하는 것\n",
    "            \n",
    "        - 강화학습을 통해 인공신경망을 학습\n",
    "            - 인공신경망으로 입력(4개의 연속적인 게임화면)이 들어오면 그 상태에서 에이전트가 할 수 있는 행동이 얼마나 좋은지를 출력\n",
    "            - 행동이 얼마나 좋은지가 행동의 가치\n",
    "            - 이 가치를 판단하는 함수를 큐함수(Q Function)라고 함\n",
    "            - 해당 논문에서는 DQN(Deep Q-Network) 인공신경망을 사용\n",
    "                - DQN으로 상태가 입력으로 들어오면 그 상태에서 제자리, 왼쪽, 오른쪽 행동의 큐함수를 출력으로 내놓음\n",
    "            - 에이전트는 큐함수에 따라서 행동\n",
    "                - 큰 가치를 지니는 행동을 선택\n",
    "            - 에이전트가 행동을 취하면 환경은 에이전트에게 보상과 다음 상태를 알려줌\n",
    "            - 에이전트는 환경과 상호작용하면서 DQN을 더 많은 보상을 받는 방향으로 조정\n",
    "        - 초반에 에이전트는 경험한 것이 적기 때문에 최적이라고 판단한 것이 진짜 최적일지 알 수 없음\n",
    "            - 학습을 계속하다가 우연히 터널을 뚫게되면 이를 통해서 에이전트는 전략을 학습\n",
    "        - 에이전트와 사람의 공통점 : 게임 화면을 보고 학습\n",
    "        - 에이전트와 사람의 차이점\n",
    "            - 에이전트는 게임의 규칙을 모름\n",
    "                - 강화학습의 장점이자 단점(느린 학습)\n",
    "            - 사람은 하나를 학습하면 다른 곳에서도 활용할 수 있음\n",
    "                - 예) 수학을 잘 배우면 과학을 배우기도 수월함\n",
    "            - 강화학습은 각 학습을 별개로 취급해서 항상 처음부터 학습해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62280e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
